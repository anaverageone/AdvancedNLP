{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install benepar"
      ],
      "metadata": {
        "id": "N8N3xj_gEWmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxWHJlJ8EP02"
      },
      "outputs": [],
      "source": [
        "import benepar\n",
        "benepar.download('benepar_en3')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "qdPEHWnGFEe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features to be extracted using spacy:\n",
        "- Dependent of target word\n",
        "- Head + full constituent from the head word\n",
        "\n",
        "Dataset: SEM-2012-SharedTask-CD-SCO-training-simple.v2"
      ],
      "metadata": {
        "id": "INuZZx8FHDgk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dependency\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import benepar"
      ],
      "metadata": {
        "id": "C6DoH9eaHQ5S"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract dependent of target word\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Sentence is taken from baskervilles08, 120th sentence\n",
        "text_1 = \"When I came round the balcony he had reached the end of the farther corridor, and I could see from the glimmer of light through an open door that he had entered one of the rooms.\"\n",
        "# Sentence is taken from baskervilles01, 40th sentence\n",
        "text_2 = \"When i said that you stimulated me I meant, to be frank, that in noting your fallacies I was occasionally guided towards the truth.\"\n",
        "\n",
        "\n",
        "doc_text1 = nlp(text_1)\n",
        "doc_text2 = nlp(text_2)\n",
        "# Iterate over to extract dependent of current token for text 1 and text 2\n",
        "for token in doc_text1:\n",
        "    print(\"Token:\", token.text)\n",
        "    print(\"Dependent:\", token.dep_, \"(\", token.head.text, \")\")\n",
        "    for child in token.children:\n",
        "        print(\"-->\", child.text, child.dep_)\n",
        "        \n",
        "for token in doc_text2:\n",
        "    print(\"Token:\", token.text)\n",
        "    print(\"Dependent:\", token.dep_, \"(\", token.head.text, \")\")\n",
        "    for child in token.children:\n",
        "        print(\"-->\", child.text, child.dep_)"
      ],
      "metadata": {
        "id": "9OiDqI7nHKRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the output into dataframe\n",
        "# Create an empty list to store the results\n",
        "results = []\n",
        "\n",
        "# Iterate over to extract dependent of current token for text 1 and text 2\n",
        "for i, doc in enumerate([doc_text1, doc_text2]):\n",
        "    for token in doc:\n",
        "        dependent = token.dep_\n",
        "        results.append({'text': f'Text {i+1}',\n",
        "                        'token': token.text,\n",
        "                        'dependent': dependent})\n",
        "\n",
        "# Convert the list of dictionaries to a pandas dataframe\n",
        "df = pd.DataFrame(results)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yi8i-aMwHax1",
        "outputId": "1b991e92-c7a2-4951-8459-8ab0dc87b0e5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      text    token dependent\n",
            "0   Text 1     When    advmod\n",
            "1   Text 1        I     nsubj\n",
            "2   Text 1     came      ROOT\n",
            "3   Text 1    round      prep\n",
            "4   Text 1      the       det\n",
            "..     ...      ...       ...\n",
            "60  Text 2   guided      ROOT\n",
            "61  Text 2  towards      prep\n",
            "62  Text 2      the       det\n",
            "63  Text 2    truth      pobj\n",
            "64  Text 2        .     punct\n",
            "\n",
            "[65 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Head + full constituent from the head word\n",
        "nlp_head = spacy.load('en_core_web_md')\n",
        "\n",
        "if spacy.__version__.startswith('2'):\n",
        "    nlp_head.add_pipe(benepar.BeneparComponent(\"benepar_en3\"))\n",
        "else:\n",
        "    nlp_head.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "\n",
        "doc_text_1 = nlp_head(text_1)\n",
        "sent1 = list(doc_text_1.sents)[0]\n",
        "\n",
        "doc_text_2 = nlp_head(text_2)\n",
        "sent2 = list(doc_text_2.sents)[0]\n",
        "\n",
        "# Save output to a dictionary\n",
        "# Text 1\n",
        "output_dict1 = {}\n",
        "output_dict1['parse_string'] = sent1._.parse_string\n",
        "output_dict1['labels'] = list(sent1._.labels)\n",
        "output_dict1['children'] = list(sent1._.children)\n",
        "print(output_dict1)\n",
        "\n",
        "# Text 2\n",
        "output_dict = {}\n",
        "output_dict['parse_string'] = sent2._.parse_string\n",
        "output_dict['labels'] = list(sent2._.labels)\n",
        "output_dict['children'] = list(sent2._.children)\n",
        "print(output_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjNucc_wE3QQ",
        "outputId": "0e7af39a-3fc4-4f08-8453-4608f1aeeeb5"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'parse_string': '(S (S (SBAR (WHADVP (WRB When)) (S (NP (PRP I)) (VP (VBD came) (PP (IN round) (NP (DT the) (NN balcony)))))) (NP (PRP he)) (VP (VBD had) (VP (VBN reached) (NP (NP (DT the) (NN end)) (PP (IN of) (NP (DT the) (JJR farther) (NN corridor))))))) (, ,) (CC and) (S (NP (PRP I)) (VP (MD could) (VP (VB see) (PP (IN from) (NP (NP (DT the) (NN glimmer)) (PP (IN of) (NP (NN light))))) (PP (IN through) (NP (DT an) (JJ open) (NN door))) (SBAR (IN that) (S (NP (PRP he)) (VP (VBD had) (VP (VBN entered) (NP (NP (CD one)) (PP (IN of) (NP (DT the) (NNS rooms))))))))))) (. .))', 'labels': ['S'], 'children': [When I came round the balcony he had reached the end of the farther corridor, ,, and, I could see from the glimmer of light through an open door that he had entered one of the rooms, .]}\n",
            "{'parse_string': '(S (SBAR (WHADVP (WRB When)) (S (NP (PRP i)) (VP (VBD said) (SBAR (IN that) (S (NP (PRP you)) (VP (VBD stimulated) (NP (PRP me)))))))) (NP (PRP I)) (VP (VBD meant) (, ,) (S (VP (TO to) (VP (VB be) (ADJP (JJ frank))))) (, ,) (SBAR (IN that) (S (PP (IN in) (S (VP (VBG noting) (NP (PRP$ your) (NNS fallacies))))) (NP (PRP I)) (VP (VBD was) (ADVP (RB occasionally)) (VP (VBN guided) (PP (IN towards) (NP (DT the) (NN truth)))))))) (. .))', 'labels': ['S'], 'children': [When i said that you stimulated me, I, meant, to be frank, that in noting your fallacies I was occasionally guided towards the truth, .]}\n"
          ]
        }
      ]
    }
  ]
}
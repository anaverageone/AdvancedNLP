{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d76953-b76b-4101-9e53-b1b8876ba7e8",
   "metadata": {},
   "source": [
    "## Feature Extraction for SRL task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e448e00a-e1f9-4db4-897d-31acdb91a6ab",
   "metadata": {},
   "source": [
    "The selected sentences are from SEM-2012-SharedTask-CD-SCO-dev-simple.v2.txt (ID=40 & 120, chapter='baskervilles08')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f65b8390-253b-4a0b-b8c9-26188bd43217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baskervilles01, 40th sentence\n",
    "sent_1 = '''When i said that you stimulated me I meant, to be frank, \n",
    "that in noting your fallacies I was occasionally guided towards the truth.'''\n",
    "# baskervilles08, 120th sentence\n",
    "sent_2 = '''When I came round the balcony he had reached the end of the farther corridor, \n",
    "and I could see from the glimmer of light through an open door that he had entered one of the rooms.'''\n",
    "sent_3 = ''' Obama was recognized as one of the greatest presidents in the USA.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5059a052-5319-4d5f-801f-4e45ad7a88d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from spacy import displacy\n",
    "import networkx as nx\n",
    "import io\n",
    "import stanza\n",
    "import benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5137f4bf-0dea-4c3e-b539-be95cb5ecc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load('en_core_web_sm')\n",
    "doc_1,doc_2, doc_3 = nlp(sent_1), nlp(sent_2), nlp(sent_3)\n",
    "df_1, df_2, df_3  = pd.DataFrame([token for token in list(doc_1) if token],columns=['Surface_form']), pd.DataFrame([token for token in list(doc_2) if token],columns=['Surface_form']), pd.DataFrame([token for token in list(doc_3) if token],columns=['Surface_form'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc08097-0129-4f03-bf7c-94f690843f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(doc):\n",
    "    '''Function that calculate the path from current tokens to the root.'''\n",
    "    edges = []\n",
    "    paths = []\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'ROOT':\n",
    "            entity1 = token.text.lower()\n",
    "        for child in token.children:\n",
    "            edges.append(('{0}'.format(token.lower_),'{0}'.format(child.lower_)))\n",
    "    graph = nx.Graph(edges)\n",
    "   \n",
    "    for token in doc:\n",
    "        entity2 = token.text.lower()\n",
    "        path_len = nx.shortest_path_length(graph, source=entity1, target=entity2)\n",
    "        paths.append(path_len)\n",
    "    return paths\n",
    "\n",
    "def create_dataframe(df, doc):\n",
    "    '''Function that display tokens and other dependency information from the original sentence.'''\n",
    "    Dependency = []\n",
    "    Head = []\n",
    "    Token_spcy = []\n",
    "    POS = []\n",
    "    NER = []\n",
    "\n",
    "    named_entities = doc.ents\n",
    "    for token in doc:\n",
    "        ne = 'NAN'\n",
    "        dependency = format(token.dep_)\n",
    "        head = token.head.text\n",
    "        token_spcy = format(token.text)\n",
    "        pos_tag = token.pos_\n",
    "        for ent in named_entities:\n",
    "            if ent.text == token.text:\n",
    "                ne = ent.label_\n",
    "            else:\n",
    "                ne = 'NAN'\n",
    "        Dependency.append(dependency)\n",
    "        Head.append(head)\n",
    "        Token_spcy.append(token_spcy)\n",
    "        POS.append(pos_tag)\n",
    "        NER.append(ne)\n",
    "   \n",
    "    df['Token_spcy'] = Token_spcy\n",
    "    df['Head'] = Head\n",
    "    df['Relation2_head'] = Dependency\n",
    "    df['Path'] = get_graph(doc)\n",
    "    df['POS'] = POS\n",
    "    df['NER'] = NER\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59a2a491-abea-42c4-9944-cf5f75ed620f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surface_form</th>\n",
       "      <th>Token_spcy</th>\n",
       "      <th>Head</th>\n",
       "      <th>Relation2_head</th>\n",
       "      <th>Path</th>\n",
       "      <th>POS</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When</td>\n",
       "      <td>When</td>\n",
       "      <td>said</td>\n",
       "      <td>advmod</td>\n",
       "      <td>1</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td>said</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>said</td>\n",
       "      <td>said</td>\n",
       "      <td>said</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>0</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>stimulated</td>\n",
       "      <td>mark</td>\n",
       "      <td>1</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you</td>\n",
       "      <td>you</td>\n",
       "      <td>stimulated</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>2</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>stimulated</td>\n",
       "      <td>stimulated</td>\n",
       "      <td>said</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>1</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>me</td>\n",
       "      <td>me</td>\n",
       "      <td>stimulated</td>\n",
       "      <td>dobj</td>\n",
       "      <td>2</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>meant</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meant</td>\n",
       "      <td>meant</td>\n",
       "      <td>stimulated</td>\n",
       "      <td>parataxis</td>\n",
       "      <td>2</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>meant</td>\n",
       "      <td>punct</td>\n",
       "      <td>3</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>be</td>\n",
       "      <td>aux</td>\n",
       "      <td>4</td>\n",
       "      <td>PART</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>meant</td>\n",
       "      <td>xcomp</td>\n",
       "      <td>3</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frank</td>\n",
       "      <td>frank</td>\n",
       "      <td>be</td>\n",
       "      <td>acomp</td>\n",
       "      <td>4</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>be</td>\n",
       "      <td>punct</td>\n",
       "      <td>3</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>said</td>\n",
       "      <td>dep</td>\n",
       "      <td>1</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>said</td>\n",
       "      <td>dobj</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>said</td>\n",
       "      <td>prep</td>\n",
       "      <td>1</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>noting</td>\n",
       "      <td>noting</td>\n",
       "      <td>in</td>\n",
       "      <td>pcomp</td>\n",
       "      <td>2</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>your</td>\n",
       "      <td>your</td>\n",
       "      <td>fallacies</td>\n",
       "      <td>poss</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>fallacies</td>\n",
       "      <td>fallacies</td>\n",
       "      <td>noting</td>\n",
       "      <td>dobj</td>\n",
       "      <td>3</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>guided</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "      <td>guided</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>3</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>occasionally</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>guided</td>\n",
       "      <td>advmod</td>\n",
       "      <td>3</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>guided</td>\n",
       "      <td>guided</td>\n",
       "      <td>noting</td>\n",
       "      <td>ccomp</td>\n",
       "      <td>2</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>towards</td>\n",
       "      <td>towards</td>\n",
       "      <td>guided</td>\n",
       "      <td>prep</td>\n",
       "      <td>3</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>truth</td>\n",
       "      <td>det</td>\n",
       "      <td>5</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>truth</td>\n",
       "      <td>truth</td>\n",
       "      <td>towards</td>\n",
       "      <td>pobj</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>said</td>\n",
       "      <td>punct</td>\n",
       "      <td>1</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Surface_form    Token_spcy        Head Relation2_head  Path    POS  NER\n",
       "0           When          When        said         advmod     1  SCONJ  NAN\n",
       "1              i             i        said          nsubj     1   PRON  NAN\n",
       "2           said          said        said           ROOT     0   VERB  NAN\n",
       "3           that          that  stimulated           mark     1  SCONJ  NAN\n",
       "4            you           you  stimulated          nsubj     2   PRON  NAN\n",
       "5     stimulated    stimulated        said          ccomp     1   VERB  NAN\n",
       "6             me            me  stimulated           dobj     2   PRON  NAN\n",
       "7              I             I       meant          nsubj     1   PRON  NAN\n",
       "8          meant         meant  stimulated      parataxis     2   VERB  NAN\n",
       "9              ,             ,       meant          punct     3  PUNCT  NAN\n",
       "10            to            to          be            aux     4   PART  NAN\n",
       "11            be            be       meant          xcomp     3    AUX  NAN\n",
       "12         frank         frank          be          acomp     4    ADJ  NAN\n",
       "13             ,             ,          be          punct     3  PUNCT  NAN\n",
       "14            \\n            \\n        said            dep     1  SPACE  NAN\n",
       "15          that          that        said           dobj     1   PRON  NAN\n",
       "16            in            in        said           prep     1    ADP  NAN\n",
       "17        noting        noting          in          pcomp     2   VERB  NAN\n",
       "18          your          your   fallacies           poss     4   PRON  NAN\n",
       "19     fallacies     fallacies      noting           dobj     3   NOUN  NAN\n",
       "20             I             I      guided      nsubjpass     1   PRON  NAN\n",
       "21           was           was      guided        auxpass     3    AUX  NAN\n",
       "22  occasionally  occasionally      guided         advmod     3    ADV  NAN\n",
       "23        guided        guided      noting          ccomp     2   VERB  NAN\n",
       "24       towards       towards      guided           prep     3    ADP  NAN\n",
       "25           the           the       truth            det     5    DET  NAN\n",
       "26         truth         truth     towards           pobj     4   NOUN  NAN\n",
       "27             .             .        said          punct     1  PUNCT  NAN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataframe(df_1, doc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3b16a6-e693-4dd4-9a03-b1bf423cf032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surface_form</th>\n",
       "      <th>Token_spcy</th>\n",
       "      <th>Head</th>\n",
       "      <th>Relation2_head</th>\n",
       "      <th>Path</th>\n",
       "      <th>POS</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When</td>\n",
       "      <td>When</td>\n",
       "      <td>came</td>\n",
       "      <td>advmod</td>\n",
       "      <td>1</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>came</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>came</td>\n",
       "      <td>came</td>\n",
       "      <td>came</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>0</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>round</td>\n",
       "      <td>round</td>\n",
       "      <td>came</td>\n",
       "      <td>prep</td>\n",
       "      <td>1</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>balcony</td>\n",
       "      <td>det</td>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>balcony</td>\n",
       "      <td>balcony</td>\n",
       "      <td>round</td>\n",
       "      <td>pobj</td>\n",
       "      <td>2</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>reached</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>reached</td>\n",
       "      <td>aux</td>\n",
       "      <td>4</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>reached</td>\n",
       "      <td>reached</td>\n",
       "      <td>balcony</td>\n",
       "      <td>relcl</td>\n",
       "      <td>3</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>end</td>\n",
       "      <td>det</td>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>end</td>\n",
       "      <td>end</td>\n",
       "      <td>reached</td>\n",
       "      <td>dobj</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>end</td>\n",
       "      <td>prep</td>\n",
       "      <td>4</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>corridor</td>\n",
       "      <td>det</td>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>farther</td>\n",
       "      <td>farther</td>\n",
       "      <td>corridor</td>\n",
       "      <td>advmod</td>\n",
       "      <td>5</td>\n",
       "      <td>ADV</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>corridor</td>\n",
       "      <td>corridor</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td>4</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>corridor</td>\n",
       "      <td>punct</td>\n",
       "      <td>5</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>\\n</td>\n",
       "      <td>\\n</td>\n",
       "      <td>came</td>\n",
       "      <td>dep</td>\n",
       "      <td>1</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>came</td>\n",
       "      <td>cc</td>\n",
       "      <td>1</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>see</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>1</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>could</td>\n",
       "      <td>could</td>\n",
       "      <td>see</td>\n",
       "      <td>aux</td>\n",
       "      <td>2</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>see</td>\n",
       "      <td>see</td>\n",
       "      <td>came</td>\n",
       "      <td>conj</td>\n",
       "      <td>1</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>from</td>\n",
       "      <td>from</td>\n",
       "      <td>see</td>\n",
       "      <td>prep</td>\n",
       "      <td>2</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>glimmer</td>\n",
       "      <td>det</td>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>glimmer</td>\n",
       "      <td>glimmer</td>\n",
       "      <td>from</td>\n",
       "      <td>pobj</td>\n",
       "      <td>3</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>glimmer</td>\n",
       "      <td>prep</td>\n",
       "      <td>4</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>light</td>\n",
       "      <td>light</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td>5</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>through</td>\n",
       "      <td>through</td>\n",
       "      <td>see</td>\n",
       "      <td>prep</td>\n",
       "      <td>2</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>an</td>\n",
       "      <td>an</td>\n",
       "      <td>door</td>\n",
       "      <td>det</td>\n",
       "      <td>4</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>open</td>\n",
       "      <td>open</td>\n",
       "      <td>door</td>\n",
       "      <td>amod</td>\n",
       "      <td>4</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>door</td>\n",
       "      <td>door</td>\n",
       "      <td>through</td>\n",
       "      <td>pobj</td>\n",
       "      <td>3</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>that</td>\n",
       "      <td>that</td>\n",
       "      <td>entered</td>\n",
       "      <td>mark</td>\n",
       "      <td>5</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>he</td>\n",
       "      <td>he</td>\n",
       "      <td>entered</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>4</td>\n",
       "      <td>PRON</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>had</td>\n",
       "      <td>had</td>\n",
       "      <td>entered</td>\n",
       "      <td>aux</td>\n",
       "      <td>4</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>entered</td>\n",
       "      <td>entered</td>\n",
       "      <td>door</td>\n",
       "      <td>relcl</td>\n",
       "      <td>4</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>entered</td>\n",
       "      <td>dobj</td>\n",
       "      <td>5</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>one</td>\n",
       "      <td>prep</td>\n",
       "      <td>4</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>rooms</td>\n",
       "      <td>det</td>\n",
       "      <td>3</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>rooms</td>\n",
       "      <td>rooms</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>see</td>\n",
       "      <td>punct</td>\n",
       "      <td>2</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Surface_form Token_spcy      Head Relation2_head  Path    POS  NER\n",
       "0          When       When      came         advmod     1  SCONJ  NAN\n",
       "1             I          I      came          nsubj     1   PRON  NAN\n",
       "2          came       came      came           ROOT     0   VERB  NAN\n",
       "3         round      round      came           prep     1    ADV  NAN\n",
       "4           the        the   balcony            det     3    DET  NAN\n",
       "5       balcony    balcony     round           pobj     2   NOUN  NAN\n",
       "6            he         he   reached          nsubj     4   PRON  NAN\n",
       "7           had        had   reached            aux     4    AUX  NAN\n",
       "8       reached    reached   balcony          relcl     3   VERB  NAN\n",
       "9           the        the       end            det     3    DET  NAN\n",
       "10          end        end   reached           dobj     4   NOUN  NAN\n",
       "11           of         of       end           prep     4    ADP  NAN\n",
       "12          the        the  corridor            det     3    DET  NAN\n",
       "13      farther    farther  corridor         advmod     5    ADV  NAN\n",
       "14     corridor   corridor        of           pobj     4    ADJ  NAN\n",
       "15            ,          ,  corridor          punct     5  PUNCT  NAN\n",
       "16           \\n         \\n      came            dep     1  SPACE  NAN\n",
       "17          and        and      came             cc     1  CCONJ  NAN\n",
       "18            I          I       see          nsubj     1   PRON  NAN\n",
       "19        could      could       see            aux     2    AUX  NAN\n",
       "20          see        see      came           conj     1   VERB  NAN\n",
       "21         from       from       see           prep     2    ADP  NAN\n",
       "22          the        the   glimmer            det     3    DET  NAN\n",
       "23      glimmer    glimmer      from           pobj     3   NOUN  NAN\n",
       "24           of         of   glimmer           prep     4    ADP  NAN\n",
       "25        light      light        of           pobj     5   NOUN  NAN\n",
       "26      through    through       see           prep     2    ADP  NAN\n",
       "27           an         an      door            det     4    DET  NAN\n",
       "28         open       open      door           amod     4    ADJ  NAN\n",
       "29         door       door   through           pobj     3   NOUN  NAN\n",
       "30         that       that   entered           mark     5   PRON  NAN\n",
       "31           he         he   entered          nsubj     4   PRON  NAN\n",
       "32          had        had   entered            aux     4    AUX  NAN\n",
       "33      entered    entered      door          relcl     4   VERB  NAN\n",
       "34          one        one   entered           dobj     5    NUM  NAN\n",
       "35           of         of       one           prep     4    ADP  NAN\n",
       "36          the        the     rooms            det     3    DET  NAN\n",
       "37        rooms      rooms        of           pobj     4   NOUN  NAN\n",
       "38            .          .       see          punct     2  PUNCT  NAN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataframe(df_2, doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aca421c-87cb-4d0c-b635-e697b891f731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Surface_form</th>\n",
       "      <th>Token_spcy</th>\n",
       "      <th>Head</th>\n",
       "      <th>Relation2_head</th>\n",
       "      <th>Path</th>\n",
       "      <th>POS</th>\n",
       "      <th>NER</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>recognized</td>\n",
       "      <td>dep</td>\n",
       "      <td>1</td>\n",
       "      <td>SPACE</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Obama</td>\n",
       "      <td>Obama</td>\n",
       "      <td>recognized</td>\n",
       "      <td>nsubjpass</td>\n",
       "      <td>1</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "      <td>recognized</td>\n",
       "      <td>auxpass</td>\n",
       "      <td>1</td>\n",
       "      <td>AUX</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>recognized</td>\n",
       "      <td>recognized</td>\n",
       "      <td>recognized</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>0</td>\n",
       "      <td>VERB</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>recognized</td>\n",
       "      <td>prep</td>\n",
       "      <td>1</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>one</td>\n",
       "      <td>one</td>\n",
       "      <td>as</td>\n",
       "      <td>pobj</td>\n",
       "      <td>2</td>\n",
       "      <td>NUM</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>of</td>\n",
       "      <td>of</td>\n",
       "      <td>one</td>\n",
       "      <td>prep</td>\n",
       "      <td>3</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>presidents</td>\n",
       "      <td>det</td>\n",
       "      <td>5</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>greatest</td>\n",
       "      <td>greatest</td>\n",
       "      <td>presidents</td>\n",
       "      <td>amod</td>\n",
       "      <td>5</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>presidents</td>\n",
       "      <td>presidents</td>\n",
       "      <td>of</td>\n",
       "      <td>pobj</td>\n",
       "      <td>4</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>presidents</td>\n",
       "      <td>prep</td>\n",
       "      <td>5</td>\n",
       "      <td>ADP</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>USA</td>\n",
       "      <td>det</td>\n",
       "      <td>5</td>\n",
       "      <td>DET</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>USA</td>\n",
       "      <td>USA</td>\n",
       "      <td>in</td>\n",
       "      <td>pobj</td>\n",
       "      <td>6</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>recognized</td>\n",
       "      <td>punct</td>\n",
       "      <td>1</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>NAN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Surface_form  Token_spcy        Head Relation2_head  Path    POS  NER\n",
       "0                            recognized            dep     1  SPACE  NAN\n",
       "1         Obama       Obama  recognized      nsubjpass     1  PROPN  NAN\n",
       "2           was         was  recognized        auxpass     1    AUX  NAN\n",
       "3    recognized  recognized  recognized           ROOT     0   VERB  NAN\n",
       "4            as          as  recognized           prep     1    ADP  NAN\n",
       "5           one         one          as           pobj     2    NUM  NAN\n",
       "6            of          of         one           prep     3    ADP  NAN\n",
       "7           the         the  presidents            det     5    DET  NAN\n",
       "8      greatest    greatest  presidents           amod     5    ADJ  NAN\n",
       "9    presidents  presidents          of           pobj     4   NOUN  NAN\n",
       "10           in          in  presidents           prep     5    ADP  NAN\n",
       "11          the         the         USA            det     5    DET  NAN\n",
       "12          USA         USA          in           pobj     6  PROPN  GPE\n",
       "13            .           .  recognized          punct     1  PUNCT  NAN"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_dataframe(df_3, doc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e099d2a-cf06-42a8-b7b7-8dd4aa1fadd0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: benepar in d:\\anaconda\\lib\\site-packages (0.2.0)\n",
      "Requirement already satisfied: nltk>=3.2 in d:\\anaconda\\lib\\site-packages (from benepar) (3.7)\n",
      "Requirement already satisfied: torch>=1.6.0 in d:\\anaconda\\lib\\site-packages (from benepar) (1.10.2)\n",
      "Requirement already satisfied: spacy>=2.0.9 in d:\\anaconda\\lib\\site-packages (from benepar) (3.3.2)\n",
      "Requirement already satisfied: protobuf in d:\\anaconda\\lib\\site-packages (from benepar) (3.20.3)\n",
      "Requirement already satisfied: torch-struct>=0.5 in d:\\anaconda\\lib\\site-packages (from benepar) (0.5)\n",
      "Requirement already satisfied: tokenizers>=0.9.4 in d:\\anaconda\\lib\\site-packages (from benepar) (0.9.4)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in d:\\anaconda\\lib\\site-packages (from benepar) (0.1.97)\n",
      "Requirement already satisfied: transformers[tokenizers,torch]>=4.2.2 in d:\\anaconda\\lib\\site-packages (from benepar) (4.2.2)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk>=3.2->benepar) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk>=3.2->benepar) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk>=3.2->benepar) (4.64.0)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk>=3.2->benepar) (1.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.23.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (0.6.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (3.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (0.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (8.0.17)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (5.2.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (3.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.27.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (1.0.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.4.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (21.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (61.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (0.7.8)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy>=2.0.9->benepar) (2.11.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy>=2.0.9->benepar) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=2.0.9->benepar) (4.1.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.0.9->benepar) (1.26.9)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->nltk>=3.2->benepar) (0.4.4)\n",
      "Requirement already satisfied: sacremoses in d:\\anaconda\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.0.43)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy>=2.0.9->benepar) (2.0.1)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from sacremoses->transformers[tokenizers,torch]>=4.2.2->benepar) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# ! pip install benepar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd7c9155-9111-448b-8556-5967a9d60c62",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting constituent-treelib"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached constituent_treelib-0.0.5-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: spacy==3.4.1 in d:\\anaconda\\lib\\site-packages (from constituent-treelib) (3.4.1)\n",
      "Collecting huspacy==0.6.0\n",
      "  Using cached huspacy-0.6.0-py3-none-any.whl (91 kB)\n",
      "Collecting protobuf==3.20.3\n",
      "  Using cached protobuf-3.20.3-cp39-cp39-win_amd64.whl (904 kB)\n",
      "Requirement already satisfied: transformers[torch]==4.2.2 in d:\\anaconda\\lib\\site-packages (from constituent-treelib) (4.2.2)\n",
      "Requirement already satisfied: benepar==0.2.0 in d:\\anaconda\\lib\\site-packages (from constituent-treelib) (0.2.0)\n",
      "Requirement already satisfied: nltk==3.7 in d:\\anaconda\\lib\\site-packages (from constituent-treelib) (3.7)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in d:\\anaconda\\lib\\site-packages (from constituent-treelib) (0.9.4)\n",
      "Collecting pdfkit==1.0.0\n",
      "  Using cached pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
      "Collecting wand==0.6.10\n",
      "  Using cached Wand-0.6.10-py2.py3-none-any.whl (142 kB)\n",
      "Requirement already satisfied: torch-struct>=0.5 in d:\\anaconda\\lib\\site-packages (from benepar==0.2.0->constituent-treelib) (0.5)\n",
      "Requirement already satisfied: torch>=1.6.0 in d:\\anaconda\\lib\\site-packages (from benepar==0.2.0->constituent-treelib) (1.10.2)\n",
      "Requirement already satisfied: sentencepiece>=0.1.91 in d:\\anaconda\\lib\\site-packages (from benepar==0.2.0->constituent-treelib) (0.1.97)\n",
      "Requirement already satisfied: packaging<22.0,>=21.3 in d:\\anaconda\\lib\\site-packages (from huspacy==0.6.0->constituent-treelib) (21.3)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from nltk==3.7->constituent-treelib) (8.0.4)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from nltk==3.7->constituent-treelib) (4.64.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in d:\\anaconda\\lib\\site-packages (from nltk==3.7->constituent-treelib) (2022.3.15)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from nltk==3.7->constituent-treelib) (1.1.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (0.6.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (1.0.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (2.27.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (3.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (2.11.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (8.1.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (0.9.1)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (61.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (2.4.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (1.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\anaconda\\lib\\site-packages (from spacy==3.4.1->constituent-treelib) (1.23.4)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from transformers[torch]==4.2.2->constituent-treelib) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in d:\\anaconda\\lib\\site-packages (from transformers[torch]==4.2.2->constituent-treelib) (0.0.43)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\anaconda\\lib\\site-packages (from packaging<22.0,>=21.3->huspacy==0.6.0->constituent-treelib) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in d:\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy==3.4.1->constituent-treelib) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy==3.4.1->constituent-treelib) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1->constituent-treelib) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1->constituent-treelib) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1->constituent-treelib) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.4.1->constituent-treelib) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->constituent-treelib) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in d:\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy==3.4.1->constituent-treelib) (0.7.8)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->nltk==3.7->constituent-treelib) (0.4.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\anaconda\\lib\\site-packages (from jinja2->spacy==3.4.1->constituent-treelib) (2.0.1)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from sacremoses->transformers[torch]==4.2.2->constituent-treelib) (1.16.0)\n",
      "Installing collected packages: protobuf, wand, pdfkit, huspacy, constituent-treelib\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "Successfully installed constituent-treelib-0.0.5 huspacy-0.6.0 pdfkit-1.0.0 protobuf-3.20.3 wand-0.6.10\n"
     ]
    }
   ],
   "source": [
    "# ! pip install constituent-treelib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36ebb4c9-af0a-46bf-a4c7-f5f87c05e6da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'en_core_web_lg' not found. Downloading...\n",
      " Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "from constituent_treelib import ConstituentTree\n",
    "# nlp= spacy.load('en_core_web_lg')\n",
    "# Define the language that should be considered with respect to the underlying benepar and spaCy models \n",
    "language = ConstituentTree.Language.English\n",
    "\n",
    "# You can also specify the desired spaCy model for the language (\"Small\" is selected by default)\n",
    "spacy_model_size = ConstituentTree.SpacyModelSize.Large\n",
    "\n",
    "# Create the neccesary NLP pipeline that is required to instantiate a ConstituentTree object\n",
    "# nlp_constituent = ConstituentTree.create_pipeline(language, spacy_model_size) \n",
    "\n",
    "# If you wish, you can instruct the library to download and install the models automatically\n",
    "nlp_constituent = ConstituentTree.create_pipeline(language, spacy_model_size, download_models=True) \n",
    "\n",
    "# Now we can instantiate a ConstituentTree object and pass it the parsed sentence as well as the NLP pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6ad150b1-f4e3-4b02-8b33-f4a423b6a131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package benepar_en3 to\n",
      "[nltk_data]     C:\\Users\\anaverageone\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
      "2023-02-16 22:29:42 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f7fbe8de6b450cac71f74c78b09cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 22:29:43 INFO: Loading these models for language: en (English):\n",
      "===========================\n",
      "| Processor    | Package  |\n",
      "---------------------------\n",
      "| tokenize     | combined |\n",
      "| pos          | combined |\n",
      "| constituency | wsj      |\n",
      "===========================\n",
      "\n",
      "2023-02-16 22:29:43 INFO: Use device: cpu\n",
      "2023-02-16 22:29:43 INFO: Loading: tokenize\n",
      "2023-02-16 22:29:43 INFO: Loading: pos\n",
      "2023-02-16 22:29:43 INFO: Loading: constituency\n",
      "2023-02-16 22:29:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "benepar.download('benepar_en3')\n",
    "nlp_stanza = stanza.Pipeline(lang='en', processors='tokenize,pos,constituency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a86698e7-4e2e-4d0d-9dc2-fc14b8ce4f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (SBAR (WHADVP (WRB When)) (S (NP (PRP i)) (VP (VBD said) (SBAR (IN that) (S (NP (PRP you)) (VP (VBD stimulated) (NP (PRP me)))))))) (NP (PRP I)) (VP (VBD meant) (, ,) (S (VP (TO to) (VP (VB be) (ADJP (JJ frank))))) (, ,) (SBAR (IN \n",
      ") (S (IN that) (S (PP (IN in) (S (VP (VBG noting) (NP (PRP$ your) (NNS fallacies))))) (NP (PRP I)) (VP (VBD was) (ADVP (RB occasionally)) (VP (VBN guided) (PP (IN towards) (NP (DT the) (NN truth))))))))) (. .))\n",
      "(S (S (SBAR (WHADVP (WRB When)) (S (NP (PRP I)) (VP (VBD came) (PP (IN round) (NP (DT the) (NN balcony)))))) (NP (PRP he)) (VP (VBD had) (VP (VBN reached) (NP (NP (DT the) (NN end)) (PP (IN of) (NP (DT the) (JJR farther) (NN corridor))))))) (, ,) (CC \n",
      ") (CC and) (S (NP (PRP I)) (VP (MD could) (VP (VB see) (PP (IN from) (NP (NP (DT the) (NN glimmer)) (PP (IN of) (NP (NN light))))) (PP (IN through) (NP (DT an) (JJ open) (NN door))) (SBAR (IN that) (S (NP (PRP he)) (VP (VBD had) (VP (VBN entered) (NP (NP (CD one)) (PP (IN of) (NP (DT the) (NNS rooms))))))))))) (. .))\n"
     ]
    }
   ],
   "source": [
    "# SPacy for constituency parsing\n",
    "nlp_benepar = spacy.load('en_core_web_md')\n",
    "nlp_benepar.add_pipe('benepar', config={'model': 'benepar_en3'})\n",
    "doc_1,doc_2 = nlp_benepar(sent_1), nlp_benepar(sent_2)\n",
    "sent_1,sent_2 = list(doc_1.sents)[0], list(doc_2.sents)[0]\n",
    "print(sent_1._.parse_string)\n",
    "print(sent_2._.parse_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d12c5b7-e669-4cce-ab4a-eca588b94033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (SBAR (WHADVP (WRB When)) (S (NP (PRP i)) (VP (VBD said) (SBAR (IN that) (S (NP (PRP you)) (VP (VBD stimulated) (NP (NP (PRP me)) (SBAR (S (NP (PRP I)) (VP (VBD meant) (, ,) (S (VP (TO to) (VP (VB be) (ADJP (JJ frank))))) (, ,) (SBAR (IN that) (S (PP (IN in) (S (VP (VBG noting) (NP (PRP$ your) (NNS fallacies))))) (NP (PRP I)) (VP (VBD was) (ADVP (RB occasionally)) (VP (VBN guided) (PP (IN towards) (NP (DT the) (NN truth)))))))))))))))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "# Stanza for constituency parsing\n",
    "doc_1 = nlp_stanza(sent_1)\n",
    "for sentence in doc_1.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "412cfcc6-c5bb-4698-8987-be380a2927fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(ROOT (S (SBAR (WHADVP (WRB When)) (S (NP (PRP I)) (VP (VBD came) (PP (IN round) (NP (DT the) (NN balcony)))))) (S (NP (PRP he)) (VP (VBD had) (VP (VBN reached) (NP (NP (DT the) (NN end)) (PP (IN of) (NP (DT the) (JJR farther) (NN corridor))))))) (, ,) (CC and) (S (NP (PRP I)) (VP (MD could) (VP (VB see) (PP (IN from) (NP (NP (DT the) (NN glimmer)) (PP (IN of) (NP (NN light))))) (PP (IN through) (NP (NP (DT an) (JJ open) (NN door)) (SBAR (WHNP (WDT that)) (S (NP (PRP he)) (VP (VBD had) (VP (VBN entered) (NP (NP (CD one)) (PP (IN of) (NP (DT the) (NNS rooms))))))))))))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "doc_2 = nlp_stanza(sent_2)\n",
    "for sentence in doc_2.sentences:\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "65a3ec86-5a70-4dfe-92ab-8ec31ea2c69f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from stanza.models.constituency.parse_tree import Tree\n",
    "\n",
    "# doc_1 = nlp_stanza(sent_1)\n",
    "# for sentence in doc_1.sentences:\n",
    "#     sent = sentence.constituency\n",
    "#     unary_depth = sent.get_constituent_counts\n",
    "#     print(type(str(sentence)))\n",
    "#     # Create a Tree object from a parse string\n",
    "#     parse_string = sent\n",
    "#     tree = Tree(parse_string)\n",
    "#     print(sentence)\n",
    "#     print()\n",
    "\n",
    "#     # Extract all the constituents\n",
    "#     constituents = [subtree for subtree in tree.subtree]\n",
    "\n",
    "#     # Print the constituents\n",
    "#     for c in constituents:\n",
    "#         print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa96d6c-612c-4dc2-9b03-807a37194624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove comment lines all together (bc there are tokens '#')\n",
    "outfile = open('../data/train_without_comments.tsv','w', encoding='utf-8')\n",
    "for line in open('../data/en_ewt-up-train.conllu','r',encoding='utf-8'):\n",
    "    if line.startswith('#'):\n",
    "        continue\n",
    "    else:\n",
    "        outfile.write(line)\n",
    "outfile.close()\n",
    "\n",
    "outfile = open('../data/test_without_comments.tsv','w', encoding='utf-8')\n",
    "for line in open('../data/en_ewt-up-test.conllu','r',encoding='utf-8'):\n",
    "    if line.startswith('#'):\n",
    "        continue\n",
    "    else:\n",
    "        outfile.write(line)\n",
    "outfile.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes there are up to 35 predicates in one sentence in the training data\n",
    "header = ['ID','FORM','LEMMA','UPOS','XPOS','FEATS','HEAD','DEPREL','DEPS','MISC','UP:PRED','UP:ARGHEADS_1','UP:ARGHEADS_2','UP:ARGHEADS_3','UP:ARGHEADS_4','UP:ARGHEADS_5','UP:ARGHEADS_6','UP:ARGHEADS_7','UP:ARGHEADS_8','UP:ARGHEADS_9','UP:ARGHEADS_10','UP:ARGHEADS_11','UP:ARGHEADS_12','UP:ARGHEADS_13','UP:ARGHEADS_14','UP:ARGHEADS_15','UP:ARGHEADS_16','UP:ARGHEADS_17','UP:ARGHEADS_18','UP:ARGHEADS_19','UP:ARGHEADS_20','UP:ARGHEADS_21','UP:ARGHEADS_22','UP:ARGHEADS_23','UP:ARGHEADS_24','UP:ARGHEADS_25','UP:ARGHEADS_26','UP:ARGHEADS_27','UP:ARGHEADS_28','UP:ARGHEADS_29','UP:ARGHEADS_30','UP:ARGHEADS_31','UP:ARGHEADS_32','UP:ARGHEADS_33','UP:ARGHEADS_34','UP:ARGHEADS_35']\n",
    "# header names taken from: \n",
    "# https://universaldependencies.org/format.html\n",
    "# https://universalpropositions.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_20876\\2145558985.py:2: ParserWarning: Falling back to the 'python' engine because ord(quotechar) > 127, meaning the quotechar is larger than one byte, and the 'c' engine does not support such quotechars; you can avoid this warning by specifying engine='python'.\n",
      "  train_df = pd.read_csv(train_path, sep='\\t', names=header, encoding='utf-8',quotechar='№')\n"
     ]
    }
   ],
   "source": [
    "train_path = '../data/train_without_comments.tsv'\n",
    "train_df = pd.read_csv(train_path, sep='\\t', names=header, encoding='utf-8',quotechar='№')\n",
    "# this quotechar is needed to avoid misinterpretation of doublequote tokens\n",
    "# print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with CopyOf= in the MISC column\n",
    "train_df = train_df[~train_df.MISC.str.contains('CopyOf=',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in predicate column with '_'\n",
    "train_df['UP:PRED'].fillna('_',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens that are at the beginning of each sentence\n",
    "start_of_sent = train_df.index[train_df['ID'] == 1 ].tolist()\n",
    "# get sentence IDs for each sentence\n",
    "def sent(row, list_firsts):\n",
    "    for ix, first in enumerate(list_firsts):\n",
    "        if row.name == first:\n",
    "            sent_num = ix+1\n",
    "            return sent_num\n",
    "\n",
    "train_df['Sent_ID'] = train_df.apply(lambda row: sent(row, start_of_sent), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the sentence number column to the front\n",
    "# adapted from: https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas\n",
    "cols = list(train_df)\n",
    "cols.insert(0,cols.pop(cols.index('Sent_ID')))\n",
    "train_df = train_df.loc[:,cols]\n",
    "# fill NaN values for all tokens that are not at the beginning of the sentence\n",
    "train_df.Sent_ID.ffill(inplace=True)\n",
    "# print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by sentences\n",
    "sentences = train_df.groupby(['Sent_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the headers needed for split data except for UP:ARGHEADS that will be added later\n",
    "header_split_df = ['Sent_ID','ID','FORM','LEMMA','UPOS','XPOS','FEATS','HEAD','DEPREL','DEPS','MISC','UP:PRED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new df to store splitted data in\n",
    "# start with the first sentence (based on the fact that we know it only has one predicate in it)\n",
    "first = sentences.get_group(1)\n",
    "pred = first['UP:PRED'].nunique()\n",
    "new_train = first.filter(items=header_split_df)\n",
    "new_train['UP:ARGHEADS'] = first['UP:ARGHEADS_1']\n",
    "# print(new_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through grouped sentences\n",
    "for name, sentence in sentences:\n",
    "    if name == 1: # skip the first sentence\n",
    "        continue\n",
    "    predicates_list = sentence['UP:PRED'].tolist()\n",
    "    if '_' in predicates_list:\n",
    "        predicates_list = [value for value in predicates_list if value != '_']\n",
    "    if not predicates_list: # if empty bc there are no predicates in the sentence\n",
    "        sentence_df = sentence.filter(items=header_split_df)\n",
    "        sentence_df['UP:ARGHEADS'] = '_'\n",
    "        new_train = pd.concat([new_train,sentence_df])\n",
    "    else:\n",
    "        predicates_dict = {k:v for k,v in enumerate(predicates_list)}\n",
    "        for ix, p in predicates_dict.items():\n",
    "            sentence_df = sentence.filter(items=header_split_df)\n",
    "            replace_dict = dict(predicates_dict)\n",
    "            del replace_dict[ix]\n",
    "            replace_list = list(replace_dict.values())\n",
    "            sentence_df['UP:ARGHEADS'] = sentence[f'UP:ARGHEADS_{ix+1}']\n",
    "            new_train = pd.concat([new_train,sentence_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the extra verbs with loc function?\n",
    "new_train.loc[new_train['UP:ARGHEADS'] != 'V', 'UP:PRED'] = '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index of the dataframe (bc it is copied for each sentence copy)\n",
    "new_train.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add copy id? aka just normal sentence id that will count the copies as sentences too\n",
    "# tokens that are at the beginning of each sentence\n",
    "start_of_sentence = new_train.index[new_train['ID'] == 1 ].tolist()\n",
    "# get sentence IDs for each sentence\n",
    "new_train['Copy_ID'] = new_train.apply(lambda row: sent(row, start_of_sentence), axis=1)\n",
    "\n",
    "# move the sentence number column to the front\n",
    "# adapted from: https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas\n",
    "cols = list(new_train)\n",
    "cols.insert(0,cols.pop(cols.index('Copy_ID')))\n",
    "new_train = new_train.loc[:,cols]\n",
    "# fill NaN values for all tokens that are not at the beginning of the sentence\n",
    "new_train.Copy_ID.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace _ with O for gold argument labels\n",
    "new_train.loc[new_train['UP:ARGHEADS'] == '_', 'UP:ARGHEADS'] = 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a tsv file\n",
    "new_train.to_csv('../data/train_split.tsv',sep='\\t',index=False,encoding='utf-8',quotechar='№')\n",
    "# this quotechar is needed to avoid misinterpretation of doublequote tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yes there are up to 35 predicates in one sentence in the training data\n",
    "header = ['ID','FORM','LEMMA','UPOS','XPOS','FEATS','HEAD','DEPREL','DEPS','MISC','UP:PRED','UP:ARGHEADS_1','UP:ARGHEADS_2','UP:ARGHEADS_3','UP:ARGHEADS_4','UP:ARGHEADS_5','UP:ARGHEADS_6','UP:ARGHEADS_7','UP:ARGHEADS_8','UP:ARGHEADS_9','UP:ARGHEADS_10','UP:ARGHEADS_11','UP:ARGHEADS_12','UP:ARGHEADS_13','UP:ARGHEADS_14','UP:ARGHEADS_15','UP:ARGHEADS_16','UP:ARGHEADS_17','UP:ARGHEADS_18','UP:ARGHEADS_19','UP:ARGHEADS_20','UP:ARGHEADS_21','UP:ARGHEADS_22','UP:ARGHEADS_23','UP:ARGHEADS_24','UP:ARGHEADS_25','UP:ARGHEADS_26','UP:ARGHEADS_27','UP:ARGHEADS_28','UP:ARGHEADS_29','UP:ARGHEADS_30','UP:ARGHEADS_31','UP:ARGHEADS_32','UP:ARGHEADS_33','UP:ARGHEADS_34','UP:ARGHEADS_35']\n",
    "# header names taken from: \n",
    "# https://universaldependencies.org/format.html\n",
    "# https://universalpropositions.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the headers needed for split data except for UP:ARGHEADS that will be added later\n",
    "header_split_df = ['Sent_ID','ID','FORM','LEMMA','UPOS','XPOS','FEATS','HEAD','DEPREL','DEPS','MISC','UP:PRED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nurbe\\AppData\\Local\\Temp\\ipykernel_20876\\361183885.py:2: ParserWarning: Falling back to the 'python' engine because ord(quotechar) > 127, meaning the quotechar is larger than one byte, and the 'c' engine does not support such quotechars; you can avoid this warning by specifying engine='python'.\n",
      "  test_df = pd.read_csv(test_path, sep='\\t', names=header,encoding='utf-8',quotechar='№')\n"
     ]
    }
   ],
   "source": [
    "test_path = '../data/test_without_comments.tsv'\n",
    "test_df = pd.read_csv(test_path, sep='\\t', names=header,encoding='utf-8',quotechar='№') \n",
    "# this quotechar is needed to avoid misinterpretation of doublequote tokens\n",
    "# print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with CopyOf= in the MISC column\n",
    "test_df = test_df[~test_df.MISC.str.contains('CopyOf=',na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace NaN values in predicate column with '_'\n",
    "test_df['UP:PRED'].fillna('_',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens that are at the beginning of each sentence\n",
    "start_of_sent_test = test_df.index[test_df['ID'] == 1 ].tolist()\n",
    "# get sentence IDs for each sentence\n",
    "def sent(row, list_firsts):\n",
    "    for ix, first in enumerate(list_firsts):\n",
    "        if row.name == first:\n",
    "            sent_num = ix+1\n",
    "            return sent_num\n",
    "\n",
    "test_df['Sent_ID'] = test_df.apply(lambda row: sent(row, start_of_sent_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move the sentence number column to the front\n",
    "# adapted from: https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas\n",
    "cols = list(test_df)\n",
    "cols.insert(0,cols.pop(cols.index('Sent_ID')))\n",
    "test_df = test_df.loc[:,cols]\n",
    "# fill NaN values for all tokens that are not at the beginning of the sentence\n",
    "test_df.Sent_ID.ffill(inplace=True)\n",
    "# print(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group data by sentences\n",
    "sentences_test = test_df.groupby(['Sent_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the new df to store splitted data in\n",
    "# start with the first sentence (based on the fact that we know it only has one predicate in it)\n",
    "first = sentences_test.get_group(1)\n",
    "pred = first['UP:PRED'].nunique()\n",
    "new_test = first.filter(items=header_split_df)\n",
    "new_test['UP:ARGHEADS'] = first['UP:ARGHEADS_1']\n",
    "# print(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through grouped sentences\n",
    "for name, sentence in sentences_test:\n",
    "    if name == 1: # skip the first sentence\n",
    "        continue\n",
    "    predicates_list = sentence['UP:PRED'].tolist()\n",
    "    if '_' in predicates_list:\n",
    "        predicates_list = [value for value in predicates_list if value != '_']\n",
    "    if not predicates_list: # if empty bc there are no predicates in the sentence\n",
    "        sentence_df = sentence.filter(items=header_split_df)\n",
    "        sentence_df['UP:ARGHEADS'] = '_'\n",
    "        new_test = pd.concat([new_test,sentence_df])\n",
    "    else:\n",
    "        predicates_dict = {k:v for k,v in enumerate(predicates_list)}\n",
    "        for ix, p in predicates_dict.items():\n",
    "            sentence_df = sentence.filter(items=header_split_df)\n",
    "            replace_dict = dict(predicates_dict)\n",
    "            del replace_dict[ix]\n",
    "            replace_list = list(replace_dict.values())\n",
    "            sentence_df['UP:ARGHEADS'] = sentence[f'UP:ARGHEADS_{ix+1}']\n",
    "            new_test = pd.concat([new_test,sentence_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the extra verbs with loc function?\n",
    "new_test.loc[new_test['UP:ARGHEADS'] != 'V', 'UP:PRED'] = '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the index of the dataframe (bc it is copied for each sentence copy)\n",
    "new_test.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add copy id? aka just normal sentence id that will count the copies as sentences too\n",
    "# tokens that are at the beginning of each sentence\n",
    "start_of_sentence = new_test.index[new_test['ID'] == 1 ].tolist()\n",
    "# get sentence IDs for each sentence\n",
    "new_test['Copy_ID'] = new_test.apply(lambda row: sent(row, start_of_sentence), axis=1)\n",
    "\n",
    "# move the sentence number column to the front\n",
    "# adapted from: https://stackoverflow.com/questions/25122099/move-column-by-name-to-front-of-table-in-pandas\n",
    "cols = list(new_test)\n",
    "cols.insert(0,cols.pop(cols.index('Copy_ID')))\n",
    "new_test = new_test.loc[:,cols]\n",
    "# fill NaN values for all tokens that are not at the beginning of the sentence\n",
    "new_test.Copy_ID.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace _ with O for gold argument labels\n",
    "new_test.loc[new_test['UP:ARGHEADS'] == '_', 'UP:ARGHEADS'] = 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to a tsv file\n",
    "new_test.to_csv('../data/test_split.tsv',sep='\\t',index=False,encoding='utf-8',quotechar='№')\n",
    "# this quotechar is needed to avoid misinterpretation of doublequote tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78a3364465fb50e0bbd2ff723d0ca9779d4996be68a7b6d285d5d9c02e4e35e6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune BERT for Token Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WARNING: Do NOT run this cell, unless you are running this on Google Colab. For a local installation run: pip install -r requirements.txt inside the terminal\n",
    "# % pip install transformers==4.9.1\n",
    "# % pip install datasets==1.11.0\n",
    "# % pip install tabulate==0.8.9\n",
    "# % pip install seqeval==1.2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert4ner/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-03-08 17:30:04.239415: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import random, time, os\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "from transformers import BertForTokenClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import logging, sys\n",
    "from transformers import BertTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Our code behind the scenes!\n",
    "import bert_utils_srl as utils\n",
    "import make_jsonl as jsonl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and convert to jsonl format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cece/Desktop/github/AdvancedNLP-Group3/Assignment 3\n"
     ]
    }
   ],
   "source": [
    "# get datafiles to jsonl format:\n",
    "train_jsonl_file, test_jsonl_file = jsonl.main(['make_jsonl.py', False, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "### ------ CHANGE MODEL NAME to bert-base-multilingual-cased ------ ###\n",
    "BERT_MODEL_NAME = \"bert-base-multilingual-cased\"\n",
    "GPU_RUN_IX=0\n",
    "\n",
    "SEED_VAL = 1234500\n",
    "SEQ_MAX_LEN = 512 #256\n",
    "PRINT_INFO_EVERY = 10 # Print status only every X batches\n",
    "GRADIENT_CLIP = 1.0\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "### ------ update training data jsonl file ------ ###\n",
    "TRAIN_DATA_PATH = train_jsonl_file #\"data/trial_mini_data.conll\" # \"data/conll2003.train.conll\"\n",
    "\n",
    "# IMPORTANT NOTE: We use as validation set the test portion, in order to avoid overfitting on the dev set, \n",
    "# and this way be able to evaluate later and compare with your previous models!\n",
    "\n",
    "### ------ update testing data jsonl file ------ ###\n",
    "DEV_DATA_PATH =  test_jsonl_file #\"data/trial_mini_data.conll\" # \"data/conll2003.dev.conll\"\n",
    "\n",
    "SAVE_MODEL_DIR = \"saved_models/MY_BERT_NER/\"\n",
    "\n",
    "LABELS_FILENAME = f\"{SAVE_MODEL_DIR}/label2index.json\"\n",
    "PRED_FILENAME = f\"{SAVE_MODEL_DIR}/pred2index.json\"\n",
    "\n",
    "LOSS_TRN_FILENAME = f\"{SAVE_MODEL_DIR}/Losses_Train_{EPOCHS}.json\"\n",
    "LOSS_DEV_FILENAME = f\"{SAVE_MODEL_DIR}/Losses_Dev_{EPOCHS}.json\"\n",
    "\n",
    "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index # -100\n",
    "\n",
    "if not os.path.exists(SAVE_MODEL_DIR):\n",
    "    os.makedirs(SAVE_MODEL_DIR)\n",
    "\n",
    "\n",
    "# Initialize Random seeds and validate if there's a GPU available...\n",
    "device, USE_CUDA = utils.get_torch_device(GPU_RUN_IX)\n",
    "random.seed(SEED_VAL)\n",
    "np.random.seed(SEED_VAL)\n",
    "torch.manual_seed(SEED_VAL)\n",
    "torch.cuda.manual_seed_all(SEED_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record everything inside a Log File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Start Logging\n"
     ]
    }
   ],
   "source": [
    "console_hdlr = logging.StreamHandler(sys.stdout)\n",
    "file_hdlr = logging.FileHandler(filename=f\"{SAVE_MODEL_DIR}/BERT_TokenClassifier_train_{EPOCHS}.log\")\n",
    "logging.basicConfig(level=logging.INFO, handlers=[console_hdlr, file_hdlr])\n",
    "logging.info(\"Start Logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:bert_utils_srl:Read 1 Sentences!\n",
      "INFO:bert_utils_srl:Read 2 Sentences!\n",
      "INFO:bert_utils_srl:Read 3 Sentences!\n",
      "INFO:bert_utils_srl:Read 4 Sentences!\n",
      "INFO:bert_utils_srl:Read 5 Sentences!\n",
      "INFO:bert_utils_srl:Read 6 Sentences!\n",
      "INFO:bert_utils_srl:Read 7 Sentences!\n",
      "INFO:bert_utils_srl:Read 8 Sentences!\n",
      "INFO:bert_utils_srl:Read 9 Sentences!\n",
      "INFO:bert_utils_srl:Read 10 Sentences!\n",
      "INFO:bert_utils_srl:Read 11 Sentences!\n",
      "INFO:bert_utils_srl:Read 12 Sentences!\n",
      "INFO:bert_utils_srl:Read 13 Sentences!\n",
      "INFO:bert_utils_srl:Read 14 Sentences!\n",
      "INFO:bert_utils_srl:Read 15 Sentences!\n",
      "INFO:bert_utils_srl:Read 16 Sentences!\n",
      "INFO:bert_utils_srl:Read 17 Sentences!\n",
      "INFO:bert_utils_srl:Read 18 Sentences!\n",
      "INFO:bert_utils_srl:Read 19 Sentences!\n",
      "INFO:bert_utils_srl:Read 20 Sentences!\n",
      "INFO:bert_utils_srl:Read 21 Sentences!\n",
      "INFO:bert_utils_srl:Read 22 Sentences!\n",
      "INFO:bert_utils_srl:Read 23 Sentences!\n",
      "INFO:bert_utils_srl:Read 24 Sentences!\n",
      "INFO:bert_utils_srl:Read 25 Sentences!\n",
      "INFO:bert_utils_srl:Read 26 Sentences!\n",
      "INFO:bert_utils_srl:Read 27 Sentences!\n",
      "INFO:bert_utils_srl:Read 28 Sentences!\n",
      "INFO:bert_utils_srl:Read 29 Sentences!\n",
      "INFO:bert_utils_srl:Read 30 Sentences!\n",
      "INFO:bert_utils_srl:Read 31 Sentences!\n",
      "INFO:bert_utils_srl:Read 32 Sentences!\n",
      "INFO:bert_utils_srl:Read 33 Sentences!\n",
      "INFO:bert_utils_srl:Read 34 Sentences!\n",
      "INFO:bert_utils_srl:Read 35 Sentences!\n",
      "INFO:bert_utils_srl:Read 36 Sentences!\n",
      "INFO:bert_utils_srl:Read 37 Sentences!\n",
      "INFO:bert_utils_srl:Read 38 Sentences!\n",
      "INFO:bert_utils_srl:Read 39 Sentences!\n",
      "INFO:bert_utils_srl:Read 40 Sentences!\n",
      "INFO:bert_utils_srl:Read 41 Sentences!\n",
      "INFO:bert_utils_srl:Read 42 Sentences!\n",
      "INFO:bert_utils_srl:Read 43 Sentences!\n",
      "INFO:bert_utils_srl:Read 44 Sentences!\n",
      "INFO:bert_utils_srl:Read 45 Sentences!\n",
      "INFO:bert_utils_srl:Read 46 Sentences!\n",
      "INFO:bert_utils_srl:Read 47 Sentences!\n",
      "INFO:bert_utils_srl:Read 48 Sentences!\n",
      "INFO:bert_utils_srl:Read 49 Sentences!\n",
      "INFO:bert_utils_srl:Read 50 Sentences!\n",
      "INFO:bert_utils_srl:Read 51 Sentences!\n",
      "INFO:bert_utils_srl:Read 52 Sentences!\n",
      "INFO:bert_utils_srl:Read 53 Sentences!\n",
      "INFO:bert_utils_srl:Read 54 Sentences!\n",
      "INFO:bert_utils_srl:Read 55 Sentences!\n",
      "INFO:bert_utils_srl:Read 56 Sentences!\n",
      "INFO:bert_utils_srl:Read 57 Sentences!\n",
      "INFO:bert_utils_srl:Read 58 Sentences!\n",
      "INFO:bert_utils_srl:Read 59 Sentences!\n",
      "INFO:bert_utils_srl:Read 60 Sentences!\n",
      "INFO:bert_utils_srl:Read 61 Sentences!\n",
      "INFO:bert_utils_srl:Read 62 Sentences!\n",
      "INFO:bert_utils_srl:Read 63 Sentences!\n",
      "INFO:bert_utils_srl:Read 64 Sentences!\n",
      "INFO:bert_utils_srl:Read 65 Sentences!\n",
      "INFO:bert_utils_srl:Read 66 Sentences!\n",
      "INFO:bert_utils_srl:Read 67 Sentences!\n",
      "INFO:bert_utils_srl:Read 68 Sentences!\n",
      "INFO:bert_utils_srl:Read 69 Sentences!\n",
      "INFO:bert_utils_srl:Read 70 Sentences!\n",
      "INFO:bert_utils_srl:Read 71 Sentences!\n",
      "INFO:bert_utils_srl:Read 72 Sentences!\n",
      "INFO:bert_utils_srl:Read 73 Sentences!\n",
      "INFO:bert_utils_srl:Read 74 Sentences!\n",
      "INFO:bert_utils_srl:Read 75 Sentences!\n",
      "INFO:bert_utils_srl:Read 76 Sentences!\n",
      "INFO:bert_utils_srl:Read 77 Sentences!\n",
      "INFO:bert_utils_srl:Read 78 Sentences!\n",
      "INFO:bert_utils_srl:Read 79 Sentences!\n",
      "INFO:bert_utils_srl:Read 80 Sentences!\n",
      "INFO:bert_utils_srl:Read 81 Sentences!\n",
      "INFO:bert_utils_srl:Read 82 Sentences!\n",
      "INFO:bert_utils_srl:Read 83 Sentences!\n",
      "INFO:bert_utils_srl:Read 84 Sentences!\n",
      "INFO:bert_utils_srl:Read 85 Sentences!\n",
      "INFO:bert_utils_srl:Read 86 Sentences!\n",
      "INFO:bert_utils_srl:Read 87 Sentences!\n",
      "INFO:bert_utils_srl:Read 88 Sentences!\n",
      "INFO:bert_utils_srl:Read 89 Sentences!\n",
      "INFO:bert_utils_srl:Read 90 Sentences!\n",
      "INFO:bert_utils_srl:Read 91 Sentences!\n",
      "INFO:bert_utils_srl:Read 92 Sentences!\n",
      "INFO:bert_utils_srl:Read 93 Sentences!\n",
      "INFO:bert_utils_srl:Read 94 Sentences!\n",
      "INFO:bert_utils_srl:Read 95 Sentences!\n",
      "INFO:bert_utils_srl:Read 96 Sentences!\n",
      "INFO:bert_utils_srl:Read 97 Sentences!\n",
      "INFO:bert_utils_srl:Read 98 Sentences!\n",
      "INFO:bert_utils_srl:Read 99 Sentences!\n",
      "INFO:bert_utils_srl:Read 100 Sentences!\n",
      "INFO:bert_utils_srl:MAX TOKENIZED SEQ LENGTH IN DATASET IS 63\n"
     ]
    }
   ],
   "source": [
    "# Initialize Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME, do_basic_tokenize=False)\n",
    "\n",
    "# Load Train Dataset\n",
    "train_data, train_labels, train_label2index, train_pred_sense, train_pred2index = utils.read_json_srl(TRAIN_DATA_PATH) #, has_labels=True)\n",
    "train_inputs, train_masks, train_labels, seq_lengths, train_pred = utils.data_to_tensors(train_data, \n",
    "                                                                            tokenizer, \n",
    "                                                                            max_len=SEQ_MAX_LEN, \n",
    "                                                                            labels=train_labels, \n",
    "                                                                            label2index=train_label2index,\n",
    "                                                                            pred_sense=train_pred_sense,\n",
    "                                                                            pred2index=train_pred2index,\n",
    "                                                                            pad_token_label_id=PAD_TOKEN_LABEL_ID)\n",
    "\n",
    "\n",
    "utils.save_label_dict(train_label2index, filename=LABELS_FILENAME)\n",
    "index2label = {v: k for k, v in train_label2index.items()}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoader for our training set.\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_pred)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:bert_utils_srl:Read 1 Sentences!\n",
      "INFO:bert_utils_srl:Read 2 Sentences!\n",
      "INFO:bert_utils_srl:Read 3 Sentences!\n",
      "INFO:bert_utils_srl:Read 4 Sentences!\n",
      "INFO:bert_utils_srl:Read 5 Sentences!\n",
      "INFO:bert_utils_srl:Read 6 Sentences!\n",
      "INFO:bert_utils_srl:Read 7 Sentences!\n",
      "INFO:bert_utils_srl:Read 8 Sentences!\n",
      "INFO:bert_utils_srl:Read 9 Sentences!\n",
      "INFO:bert_utils_srl:Read 10 Sentences!\n",
      "INFO:bert_utils_srl:Read 11 Sentences!\n",
      "INFO:bert_utils_srl:Read 12 Sentences!\n",
      "INFO:bert_utils_srl:Read 13 Sentences!\n",
      "INFO:bert_utils_srl:Read 14 Sentences!\n",
      "INFO:bert_utils_srl:Read 15 Sentences!\n",
      "INFO:bert_utils_srl:Read 16 Sentences!\n",
      "INFO:bert_utils_srl:Read 17 Sentences!\n",
      "INFO:bert_utils_srl:MAX TOKENIZED SEQ LENGTH IN DATASET IS 43\n"
     ]
    }
   ],
   "source": [
    "# Load Dev Dataset\n",
    "dev_data, dev_labels, _ , dev_pred_sense, _ = utils.read_json_srl(DEV_DATA_PATH) #, has_labels=True)\n",
    "dev_inputs, dev_masks, dev_labels, dev_lens, dev_pred = utils.data_to_tensors(dev_data, \n",
    "                                                                    tokenizer, \n",
    "                                                                    max_len=SEQ_MAX_LEN, \n",
    "                                                                    labels=dev_labels, \n",
    "                                                                    label2index=train_label2index,\n",
    "                                                                    pred_sense=dev_pred_sense,\n",
    "                                                                    pred2index=train_pred2index,\n",
    "                                                                    pad_token_label_id=PAD_TOKEN_LABEL_ID)  \n",
    "\n",
    "# Create the DataLoader for our Development set.\n",
    "dev_data = TensorDataset(dev_inputs, dev_masks, dev_labels, dev_pred) \n",
    "dev_sampler = RandomSampler(dev_data)     \n",
    "dev_dataloader = DataLoader(dev_data, sampler=dev_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(BERT_MODEL_NAME, num_labels=len(train_label2index))\n",
    "model.config.finetuning_task = 'token-classification'\n",
    "model.config.id2label = index2label  \n",
    "model.config.label2id = train_label2index  \n",
    "if USE_CUDA: model.cuda()\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# Create optimizer and the learning rate scheduler.\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, eps=1e-8)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                        num_warmup_steps=0,\n",
    "                                        num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cycle (Fine-tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "INFO:root:======== Epoch 1 / 2 ========\n",
      "INFO:root:Training...\n",
      "start processing train_dataloader ......\n",
      "INFO:root:  Batch    10  of     25.    Elapsed: 0:03:14.    Loss: 0.6744903326034546.\n",
      "INFO:root:  Batch    20  of     25.    Elapsed: 0:05:56.    Loss: 0.5470535159111023.\n",
      "calculating the avg loss ......\n",
      "log the loss value  ......\n",
      "INFO:root:\n",
      "INFO:root:  Average training loss: 1.1210\n",
      "INFO:root:  Training Epoch took: 0:07:02\n",
      "starts validation ......\n",
      "INFO:bert_utils_srl:***** Running evaluation Validation Set *****\n",
      "INFO:bert_utils_srl:  Batch size = 4\n",
      "INFO:bert_utils_srl:{0: 'O', 1: 'B-ARG0', 2: 'B-V', 3: 'B-ARG1', 4: 'B-ARGM-LOC', 5: 'B-ARGM-MOD', 6: 'B-ARGM-GOL', 7: 'B-ARGM-TMP', 8: 'B-ARG2', 9: 'B-ARGM-ADV', 10: 'B-ARGM-MNR', 11: 'B-ARGM-LVB', 12: 'B-ARGM-PRR', 13: 'B-ARG4', 14: 'B-ARGM-NEG', 15: 'B-ARGM-EXT', 16: 'B-R-ARG0', 17: 'B-ARGM-ADJ', 18: 'B-R-ARG1', 19: 'B-C-V', 20: 'B-ARG3', 21: 'B-ARGM-COM', 22: 'B-ARGM-DIS', 23: 'B-ARGM-CAU'}\n",
      "INFO:root:  Validation Loss: 0.63\n",
      "INFO:root:  Precision: 0.00 || Recall: 0.00 || F1: 0.00\n",
      "INFO:root:  Validation took: 0:00:21\n",
      "INFO:bert_utils_srl:Saving model to saved_models/MY_BERT_NER//EPOCH_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert4ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:\n",
      "INFO:root:======== Epoch 2 / 2 ========\n",
      "INFO:root:Training...\n",
      "start processing train_dataloader ......\n",
      "INFO:root:  Batch    10  of     25.    Elapsed: 0:02:58.    Loss: 0.502220094203949.\n",
      "INFO:root:  Batch    20  of     25.    Elapsed: 0:05:40.    Loss: 0.3539716601371765.\n",
      "calculating the avg loss ......\n",
      "log the loss value  ......\n",
      "INFO:root:\n",
      "INFO:root:  Average training loss: 0.5756\n",
      "INFO:root:  Training Epoch took: 0:06:49\n",
      "starts validation ......\n",
      "INFO:bert_utils_srl:***** Running evaluation Validation Set *****\n",
      "INFO:bert_utils_srl:  Batch size = 4\n",
      "INFO:bert_utils_srl:{0: 'O', 1: 'B-ARG0', 2: 'B-V', 3: 'B-ARG1', 4: 'B-ARGM-LOC', 5: 'B-ARGM-MOD', 6: 'B-ARGM-GOL', 7: 'B-ARGM-TMP', 8: 'B-ARG2', 9: 'B-ARGM-ADV', 10: 'B-ARGM-MNR', 11: 'B-ARGM-LVB', 12: 'B-ARGM-PRR', 13: 'B-ARG4', 14: 'B-ARGM-NEG', 15: 'B-ARGM-EXT', 16: 'B-R-ARG0', 17: 'B-ARGM-ADJ', 18: 'B-R-ARG1', 19: 'B-C-V', 20: 'B-ARG3', 21: 'B-ARGM-COM', 22: 'B-ARGM-DIS', 23: 'B-ARGM-CAU'}\n",
      "INFO:root:  Validation Loss: 0.69\n",
      "INFO:root:  Precision: 0.00 || Recall: 0.00 || F1: 0.00\n",
      "INFO:root:  Validation took: 0:00:25\n",
      "INFO:bert_utils_srl:Saving model to saved_models/MY_BERT_NER//EPOCH_2\n",
      "INFO:root:\n",
      "INFO:root:Training complete!\n"
     ]
    }
   ],
   "source": [
    "loss_trn_values, loss_dev_values = [], []\n",
    "\n",
    "for epoch_i in range(1, EPOCHS+1):\n",
    "    # Perform one full pass over the training set.\n",
    "    logging.info(\"\")\n",
    "    logging.info('======== Epoch {:} / {:} ========'.format(epoch_i, EPOCHS))\n",
    "    logging.info('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    print(\"start processing train_dataloader ......\")\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[2].to(device)\n",
    "        b_preds = batch[3].to(device) ###### added predicate ######\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Perform a forward pass (evaluate the model on this training batch).\n",
    "        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels, token_type_ids=b_preds) #input_ids = \n",
    "        ### added token_tupe_ids = b_preds, also 'input_ids')\n",
    "        loss = outputs[0]\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRADIENT_CLIP)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Progress update\n",
    "        if step % PRINT_INFO_EVERY == 0 and step != 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = utils.format_time(time.time() - t0)\n",
    "            # Report progress.\n",
    "            logging.info('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.    Loss: {}.'.format(step, len(train_dataloader),\n",
    "                                                                                            elapsed, loss.item()))\n",
    "\n",
    "    # Calculate the average loss over the training data.\n",
    "    print(\"calculating the avg loss ......\")\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    print(\"log the loss value  ......\")\n",
    "    loss_trn_values.append(avg_train_loss)\n",
    "\n",
    "    logging.info(\"\")\n",
    "    logging.info(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
    "    logging.info(\"  Training Epoch took: {:}\".format(utils.format_time(time.time() - t0)))\n",
    "\n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on our validation set.\n",
    "    print(\"starts validation ......\")\n",
    "    t0 = time.time()\n",
    "    results, preds_list = utils.evaluate_bert_model(dev_dataloader, BATCH_SIZE, model, tokenizer, index2label, PAD_TOKEN_LABEL_ID, prefix=\"Validation Set\")\n",
    "    \n",
    "    loss_dev_values.append(results['loss'])\n",
    "    logging.info(\"  Validation Loss: {0:.2f}\".format(results['loss']))\n",
    "    logging.info(\"  Precision: {0:.2f} || Recall: {1:.2f} || F1: {2:.2f}\".format(results['precision']*100, results['recall']*100, results['f1']*100))\n",
    "    logging.info(\"  Validation took: {:}\".format(utils.format_time(time.time() - t0)))\n",
    "\n",
    "\n",
    "    # Save Checkpoint for this Epoch\n",
    "    utils.save_model(f\"{SAVE_MODEL_DIR}/EPOCH_{epoch_i}\", {\"args\":[]}, model, tokenizer)\n",
    "\n",
    "\n",
    "utils.save_losses(loss_trn_values, filename=LOSS_TRN_FILENAME)\n",
    "utils.save_losses(loss_dev_values, filename=LOSS_DEV_FILENAME)\n",
    "logging.info(\"\")\n",
    "logging.info(\"Training complete!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------\n",
    "# SCRIPT NOT UPDATED BEYOND THIS CELL\n",
    "-------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Fine-tuned Model for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from torch.utils.data import SequentialSampler\n",
    "\n",
    "GPU_IX=0\n",
    "device, USE_CUDA = utils.get_torch_device(GPU_IX)\n",
    "FILE_HAS_GOLD = True\n",
    "SEQ_MAX_LEN = 256\n",
    "BATCH_SIZE = 4\n",
    "# IMPORTANT NOTE: We predict on the dev set to make the results comparable with your previous models from this course\n",
    "TEST_DATA_PATH = test_jsonl_file #\"data/trial_mini_data.conll\" # \"data/conll2003.dev.conll\"\n",
    "# TEST_DATA_PATH = \"data/trial_unk_data.conll\"\n",
    "MODEL_DIR = \"saved_models/MY_BERT_NER/\"\n",
    "LOAD_EPOCH = 1\n",
    "INPUTS_PATH=f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}/model_inputs.txt\"\n",
    "OUTPUTS_PATH=f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}/model_outputs.txt\"\n",
    "PAD_TOKEN_LABEL_ID = CrossEntropyLoss().ignore_index # -100\n",
    "\n",
    "console_hdlr = logging.StreamHandler(sys.stdout)\n",
    "file_hdlr = logging.FileHandler(filename=f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}/BERT_TokenClassifier_predictions.log\")\n",
    "logging.basicConfig(level=logging.INFO, handlers=[console_hdlr, file_hdlr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = utils.load_model(BertForTokenClassification, BertTokenizer, f\"{MODEL_DIR}/EPOCH_{LOAD_EPOCH}\")\n",
    "label2index = utils.load_label_dict(f\"{MODEL_DIR}/label2index.json\")\n",
    "index2label = {v:k for k,v in label2index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File for Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:bert_utils_srl:Read 1 Sentences!\n",
      "INFO:bert_utils_srl:Read 2 Sentences!\n",
      "INFO:bert_utils_srl:Read 3 Sentences!\n",
      "INFO:bert_utils_srl:Read 4 Sentences!\n",
      "INFO:bert_utils_srl:Read 5 Sentences!\n",
      "INFO:bert_utils_srl:Read 6 Sentences!\n",
      "INFO:bert_utils_srl:Read 7 Sentences!\n",
      "INFO:bert_utils_srl:Read 8 Sentences!\n",
      "INFO:bert_utils_srl:Read 9 Sentences!\n",
      "INFO:bert_utils_srl:Read 10 Sentences!\n",
      "INFO:bert_utils_srl:Read 11 Sentences!\n",
      "INFO:bert_utils_srl:Read 12 Sentences!\n",
      "INFO:bert_utils_srl:Read 13 Sentences!\n",
      "INFO:bert_utils_srl:Read 14 Sentences!\n",
      "INFO:bert_utils_srl:Read 15 Sentences!\n",
      "INFO:bert_utils_srl:Read 16 Sentences!\n",
      "INFO:bert_utils_srl:Read 17 Sentences!\n",
      "INFO:bert_utils_srl:MAX TOKENIZED SEQ LENGTH IN DATASET IS 43\n"
     ]
    }
   ],
   "source": [
    "test_data, test_labels, _, test_pred_sense, _ = utils.read_json_srl(TEST_DATA_PATH)# , has_labels=FILE_HAS_GOLD)\n",
    "prediction_inputs, prediction_masks, gold_labels, seq_lens, gold_pred = utils.data_to_tensors(test_data, \n",
    "                                                                                   tokenizer, \n",
    "                                                                                   max_len=SEQ_MAX_LEN, \n",
    "                                                                                   labels=test_labels, \n",
    "                                                                                   label2index=label2index,\n",
    "                                                                                   pred2index=train_pred2index,\n",
    "                                                                                   pred_sense=test_pred_sense)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Predicting labels for 17 test sentences...\n",
      "INFO:bert_utils_srl:***** Running evaluation Test Set *****\n",
      "INFO:bert_utils_srl:  Batch size = 4\n",
      "INFO:bert_utils_srl:{0: 'O', 1: 'B-ARG0', 2: 'B-V', 3: 'B-ARG1', 4: 'B-ARGM-LOC', 5: 'B-ARGM-MOD', 6: 'B-ARGM-GOL', 7: 'B-ARGM-TMP', 8: 'B-ARG2', 9: 'B-ARGM-ADV', 10: 'B-ARGM-MNR', 11: 'B-ARGM-LVB', 12: 'B-ARGM-PRR', 13: 'B-ARG4', 14: 'B-ARGM-NEG', 15: 'B-ARGM-EXT', 16: 'B-R-ARG0', 17: 'B-ARGM-ADJ', 18: 'B-R-ARG1', 19: 'B-C-V', 20: 'B-ARG3', 21: 'B-ARGM-COM', 22: 'B-ARGM-DIS', 23: 'B-ARGM-CAU'}\n",
      "INFO:bert_utils_srl:\n",
      "----- 1 -----\n",
      "['What', 'if', 'Google', 'Morphed', 'Into', 'GoogleOS', '?']\n",
      "\n",
      "GOLD: ['O', 'O', 'B-ARG1', 'B-V', 'O', 'B-ARG2', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 2 -----\n",
      "['What', 'if', 'Google', 'expanded', 'on', 'its', 'search', '-', 'engine', '(', 'and', 'now', 'e-mail', ')', 'wares', 'into', 'a', 'full', '-', 'fledged', 'operating', 'system', '?']\n",
      "\n",
      "GOLD: ['O', 'O', 'B-ARG0', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG4', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 3 -----\n",
      "['[', 'via', 'Microsoft', 'Watch', 'from', 'Mary', 'Jo', 'Foley', ']']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 4 -----\n",
      "['(', 'And', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 5 -----\n",
      "['(', 'And', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'B-ARGM-DIS', 'O', 'B-V', 'B-ARG1', 'O', 'O', 'O', 'O', 'B-ARG2', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 6 -----\n",
      "['(', 'And', ',', 'by', 'the', 'way', ',', 'is', 'anybody', 'else', 'just', 'a', 'little', 'nostalgic', 'for', 'the', 'days', 'when', 'that', 'was', 'a', 'good', 'thing', '?', ')']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARGM-TMP', 'B-ARG1', 'B-V', 'O', 'O', 'B-ARG2', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 7 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'B-ARG2', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 8 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'B-ARG0', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 9 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'B-V', 'O', 'B-ARG2', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 10 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'O', 'O', 'B-ARGM-MOD', 'B-V', 'O', 'O', 'O', 'O', 'O', 'B-ARGM-ADV', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 11 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 12 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'B-ARG0', 'O', 'B-ARGM-ADV', 'B-V', 'B-ARGM-TMP', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 13 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG1', 'B-V', 'B-ARGM-ADV', 'O', 'O', 'B-ARG2', 'O', 'O', 'B-ARGM-LOC', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 14 -----\n",
      "['This', 'BuzzMachine', 'post', 'argues', 'that', 'Google', \"'s\", 'rush', 'toward', 'ubiquity', 'might', 'backfire', '--', 'which', 'we', \"'ve\", 'all', 'heard', 'before', ',', 'but', 'it', \"'s\", 'particularly', 'well', '-', 'put', 'in', 'this', 'post', '.']\n",
      "\n",
      "GOLD: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 15 -----\n",
      "['Google', 'is', 'a', 'nice', 'search', 'engine', '.']\n",
      "\n",
      "GOLD: ['B-ARG1', 'B-V', 'O', 'O', 'O', 'B-ARG2', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 16 -----\n",
      "['Does', 'anybody', 'use', 'it', 'for', 'anything', 'else', '?']\n",
      "\n",
      "GOLD: ['B-V', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "----- 17 -----\n",
      "['Does', 'anybody', 'use', 'it', 'for', 'anything', 'else', '?']\n",
      "\n",
      "GOLD: ['O', 'B-ARG0', 'B-V', 'B-ARG1', 'O', 'B-ARG2', 'O', 'O']\n",
      "PRED:['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "INFO:bert_utils_srl:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        ARG0       0.00      0.00      0.00         4\n",
      "        ARG1       0.00      0.00      0.00        11\n",
      "        ARG2       0.00      0.00      0.00         8\n",
      "        ARG4       0.00      0.00      0.00         1\n",
      "    ARGM-ADV       0.00      0.00      0.00         3\n",
      "    ARGM-DIS       0.00      0.00      0.00         1\n",
      "    ARGM-LOC       0.00      0.00      0.00         1\n",
      "    ARGM-MOD       0.00      0.00      0.00         1\n",
      "    ARGM-TMP       0.00      0.00      0.00         2\n",
      "           V       0.00      0.00      0.00        16\n",
      "\n",
      "   micro avg       0.00      0.00      0.00        48\n",
      "   macro avg       0.00      0.00      0.00        48\n",
      "weighted avg       0.00      0.00      0.00        48\n",
      "\n",
      "INFO:root:  Test Loss: 1.03\n",
      "INFO:root:  Precision: 0.00 || Recall: 0.00 || F1: 0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bert4ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/envs/bert4ner/lib/python3.8/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "if FILE_HAS_GOLD:\n",
    "    prediction_data = TensorDataset(prediction_inputs, prediction_masks, gold_labels, gold_pred)\n",
    "    prediction_sampler = SequentialSampler(prediction_data)\n",
    "    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=BATCH_SIZE)\n",
    "\n",
    "    logging.info('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
    "                                                                                            \n",
    "    results, preds_list = utils.evaluate_bert_model(prediction_dataloader, BATCH_SIZE, model, tokenizer, index2label, \n",
    "                                                        PAD_TOKEN_LABEL_ID, full_report=True, prefix=\"Test Set\")\n",
    "    logging.info(\"  Test Loss: {0:.2f}\".format(results['loss']))\n",
    "    logging.info(\"  Precision: {0:.2f} || Recall: {1:.2f} || F1: {2:.2f}\".format(results['precision']*100, results['recall']*100, results['f1']*100))\n",
    "\n",
    "    with open(OUTPUTS_PATH, \"w\") as fout:\n",
    "        with open(INPUTS_PATH, \"w\") as fin:\n",
    "            for sent, pred in preds_list:\n",
    "                fin.write(\" \".join(sent)+\"\\n\")\n",
    "                fout.write(\" \".join(pred)+\"\\n\")\n",
    "\n",
    "else:\n",
    "    # https://huggingface.co/transformers/main_classes/pipelines.html#transformers.TokenClassificationPipeline\n",
    "    logging.info('Predicting labels for {:,} test sentences...'.format(len(test_data)))\n",
    "    if not USE_CUDA: GPU_IX = -1\n",
    "    nlp = pipeline('token-classification', model=model, tokenizer=tokenizer, device=GPU_IX)\n",
    "    nlp.ignore_labels = []\n",
    "    with open(OUTPUTS_PATH, \"w\") as fout:\n",
    "        with open(INPUTS_PATH, \"w\") as fin:\n",
    "            for seq_ix, seq in enumerate(test_data):\n",
    "                sentence = \" \".join(seq)\n",
    "                predicted_labels = []\n",
    "                output_obj = nlp(sentence)\n",
    "                # [print(o) for o in output_obj]\n",
    "                for tok in output_obj:\n",
    "                    if '##' not in tok['word']:\n",
    "                        predicted_labels.append(tok['entity'])\n",
    "                logging.info(f\"\\n----- {seq_ix+1} -----\\n{seq}\\nPRED:{predicted_labels}\")\n",
    "                fin.write(sentence+\"\\n\")\n",
    "                fout.write(\" \".join(predicted_labels)+\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
